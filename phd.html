<!DOCTYPE html>
<meta charset='UTF-8' name="viewport" content="width=device-width, initial-scale=1">
<html>
<head>
    <link href="css/styles.css" rel="stylesheet">
	<!-- Grid -->	
	<style>
	.site {
		display: grid;
		grid-template-columns: 3.5fr 2.5fr 4fr 1fr;
		grid-template-rows: auto;
		grid-template-areas:
			"abstract abstract teaser teaser"
			"abstract2 abstract2 abstract2 . "
			"awehead awehead awehead awehead"
			"sechead sechead sechead sechead"
		    "fig1 fig1 txt1 txt1"
			"txt12 txt12 txt12 txt12"
		    "txt2 txt2 txt2 txt2";
		align-items: center;
		margin: 0 35px 35px 35px;
	}
	.figGrid{
		display: grid;
		grid-template-columns: auto auto auto;
		grid-template-rows: auto;
		grid-template-areas:
			"area1 area2 area3";
		grid-column-gap: 1em;
		align-items: center;
	}
	.figure{
		padding: 0 1em 0 1em;
		text-align: center;
	}

	@media (max-width: 900px){
	  .site {
		  display: grid;
		  grid-template-columns: auto;
		  grid-template-rows: auto;
		  grid-template-areas:
		      "abstract"
			  "abstract2"
		      "teaser"
			  "awehead"
			  "sechead"
			  "fig1"
		      "txt1"
		      "txt12"
		      "txt2";
		font-size: 10pt;
	  }
	  
	  h1 {
			margin: 0;
			font-weight: 300;
			font-size: 20pt;
	  }
		.figGrid{
			grid-template-columns: auto;
			grid-template-areas:
				"area1"
				"area2"
				"area3";
			margin: 0 5% 0 5%;
			grid-row-gap: 1em;
		}
		.figure{
			padding: 0 5% 0 5%;
			text-align: center;
		}

	}
	@media (min-width: 1500px){
	  .site {
		  display: grid;
		  grid-template-columns: 1fr 6fr 0.5fr 4.5fr 1fr;
		  grid-template-rows: auto;
		  grid-template-areas:
		      "abstract abstract . teaser teaser"
			  "abstract2 abstract2 . teaser teaser"
			  ". awehead awehead awehead ."
			  ". sechead sechead sechead ."
		      ". fig1 txt1 txt1 ."
			  ". txt12 txt12 txt12 ."
		      ". txt2 txt2 txt2 ."
	  }
	  margin: 0 35px 35px 35px;
	}


	.abs {
		padding: 0 1em 0 1em;
	}
	.site .header {
		padding: 0 1em 0 1em;
	}
	.site .content {
		grid-area: txt1;
		padding: 0 1em 0 1em;
	}
	.site p {
		text-align: justify;
	}
	</style>
    <title>Personalized Highlight Detection</title>

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600" rel="stylesheet">
</head>


<body>
  <div text-align='left'>
    <a href="https://garciadelmolino.github.io">Home</a>
  </div>
  <section class="site">
      <div class="abs" style="grid-area: abstract;">
		<h1>Personalized Highlight Detection for Automatic GIF Creation</h1>
  	    <hr>
        <div class='nav-links' text-align='center'>
                <a href="http://arxiv.org/abs/1804.06604">Paper</a>
                - <a href="https://github.com/gyglim/personalized-highlights-dataset">Dataset</a>
				- <a href="files/PHD-GIFs.bib">BibTeX</a>
        </div> 
		
		<p> Highlight detection models are typically trained to identify cues
		that make visual content appealing or interesting for the general
		public, with the objective of reducing a video to such moments.
		However, this “interestingness” of a video segment or image is
		subjective. Thus, such highlight models provide results of limited
		relevance for the individual user. On the other hand, training one
		model per user is inefficient and requires large amounts of personal
		information which is typically not available. To overcome these
		limitations, we present a global ranking model which can condition
		on a particular user’s interests. Rather than training one model per
		user, our model is personalized via its inputs, which allows it to
		effectively adapt its predictions, given only a few of user-specific
		examples. To train this model, we create a large-scale dataset of
		users and the GIFs they created, giving us an accurate indication
		of their interests.  
	  </div>     

      <div class="abs" style="grid-area: abstract2;">
		<p> Our experiments show that using the user history
		substantially improves the prediction accuracy. On our test set of
		850 videos, our model improves the recall over 8% with respect to
		generic highlight detectors. Our method proves more precise than the
		user-agnostic baselines even with just one person-specific example.
      </div>     
      	<div class="figure" style="grid-area: teaser;">
      	<img src="files/PHD-GIFs.png" width="100%">
      </div>

	<div class='header' style='grid-area: awehead;'>
		<hr>
		<h2 text-align='left'>The awesomness of PHD-GIFs: </h2>
		<section class="figGrid">
			<div class="figure" style="grid-area: area1;">
				<img src="files/boxing.png" width="100%">
			</div>
			<div class="figure" style="grid-area: area2;">
				<img src="files/trees.png" width="100%">
			</div>	
		</section>
	</div>	  
	<div class='header' style='grid-area: sechead;'>
		<hr>
		<h2 text-align='left'>Method Overview </h2>
	</div>
	<div class="figure" style="grid-area: fig1;">
		<img src="files/PHDarch.png" width="100%">
	</div>	
	<div class='content' style='grid-area: txt1;'>
		<p> We propose a model that predicts the score of a segment
		as a function of both the segment itself and the user’s previously
		selected highlights. As such, the model learns to take into account
		the user history to make accurate personalized predictions.</p> 

		<p>
		While there are several ways to do personalization, making the
		user history an input to the model has the advantage that a single
		model is sufficient and that the model can use all annotations
		from all users in training. A single model can predict personalized
		highlights for all users and new user information can trivially be included.
		</p> 
	</div>	
	<div class='content' style='grid-area: txt12;'>
		<p>
		We propose two models for for our ranking objective, which are combined with late
		fusion. One takes the segment representation and aggregated history
		as input (the FNN model), while the second directly uses the distances
		between the segments and the history (the SVM model).</p> 
	</div>

	<div class='content' style='grid-area: txt2;'>
		<hr>
		<h2 text-align='left'>Experimental Results </h2>

            <p>When analyzing the results, we find that our
				method outperforms the baselines by a significant
				margin.
				Models using only generic highlight information or only the similarity
				to previous GIFs perform similar (15.86% for Video2GIF
				(ours) vs. 15.64% mAP for SVM-D), despite the simplicity of the
				distance model. Thus, we can conclude that these two kinds of information
				are both important and that there is a lot of signal contained
				in a user’s history about his future choice of highlights.
                </p>
		<section class="figGrid">
			<div class="figure" style="grid-area: area1;">
				<img src="files/PHDmap.png" width="100%">
			</div>
			<div class="figure" style="grid-area: area2;">
				<img src="files/PHDmsd.png" width="100%">
			</div>	
			<div class="figure" style="grid-area: area3;">
				<img src="files/PHDrec.png" width="100%">
			</div>	
		</section>
		<div class="figure"><img src="files/PHDlegend.png" width="70%"></div>
	</div>
  </section>
</body>
</html>

