<!DOCTYPE html>
<meta charset='UTF-8' name="viewport" content="width=device-width, initial-scale=1">
<html>
<head>
    <link href="css/styles.css" rel="stylesheet">
	<!-- Grid -->	
	<style>
	.paper_site {
		grid-template-columns: 4fr 2fr 5fr;
		grid-template-areas:
			"abstract abstract teaser"
			"abstract2 abstract2 abstract2"
			"sechead sechead sechead"
		    "fig1 txt1 txt1"
		    "txt2 txt2 txt2"
	}

	.figGrid{
		grid-template-columns: 3fr 1.2fr 3fr;
	}

	@media (max-width: 900px){
		.paper_site {
		  grid-template-columns: 1fr 4fr 1fr;
		  grid-template-areas:
			  "abstract abstract abstract"
			  "abstract2 abstract2 abstract2"
			  "teaser teaser teaser"
			  "sechead sechead sechead"
			  ". fig1 ."
			  "txt1 txt1 txt1"
			  "txt2 txt2 txt2";
		  font-size: 10pt;
		}
	  
		h1 {
			margin: 0;
			font-weight: 300;
			font-size: 20pt;
		}
		.figGrid{
			grid-template-columns: 1fr 1.5fr 1fr;
			grid-template-areas:
				"area1 area1 area1"
				"area3 area3 area3"
				". area2 .";
			margin: 0 5% 0 5%;
			grid-row-gap: 1em;
		}
		.figure{
			padding: 0 5% 0 5%;
			text-align: center;
		}
	}

	@media (min-width: 1200px){
	  body {
		  width: 1020px;
	  }
	}

	</style>
    <title>Active Video Summarization</title>

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600" rel="stylesheet">
</head>


<body>
  <div text-align='left'>
    <a href="https://garciadelmolino.github.io">Home</a>
  </div>
  <section class="paper_site">
      <div class="abs" style="grid-area: abstract;">
		<h1>Active Video Summarization: </h1>
		<h1>Customized Summaries via On-line Interaction with the User.</h1>
  	    <hr>
        <div class='nav-links'>
                <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14856/14279">Paper</a>
                - <a href="https://www.youtube.com/watch?v=jLDe1uB2vj4">Demo</a>
                - <a href="https://github.com/GarciaDelMolino/AVS">Code</a>
		- <a href="files/AVS.bib">BibTeX</a></p>
        </div> 
		
		<p> To facilitate the browsing of long videos, automatic video
		summarization provides an excerpt that represents its content.
		In the case of egocentric and consumer videos, due
		to their personal nature, adapting the summary to specific
		user’s preferences is desirable. Current approaches to customizable
		video summarization obtain the user’s preferences
		prior to the summarization process. As a result, the user needs
		to manually modify the summary to further meet the preferences.
		In this paper, we introduce Active Video Summarization
		(AVS), an interactive approach to gather the user’s
		preferences while creating the summary. AVS asks questions
		about the summary to update it on-line until the user is satisfied.
		To minimize the interaction, the best segment to inquire
		next is inferred from the previous feedback. We evaluate
		AVS in the commonly used UTEgo dataset. We also
		introduce a new dataset for customized video summarization
		(CSumm) recorded with a Google Glass. 
	  </div>     

      <div class="abs" style="grid-area: abstract2;">
		<p> The results
		show that AVS achieves an excellent compromise between
		usability and quality. In 41% of the videos, AVS is considered
		the best over all tested baselines, including summaries manually
		generated. Also, when looking for specific events in the
		video, AVS provides an average level of satisfaction higher
		than those of all other baselines after only six questions to the
		user.
      </div>     
      	<div class="figure" style="grid-area: teaser;">
      	<img src="files/AVS.png" width="100%">
      </div>

	<div class='header' style='grid-area: sechead;'>
		<hr>
		<h2 text-align='left'>Overview of Active Video Summarization </h2>
	</div>
	<div class="figure" style="grid-area: fig1;">
		<img src="files/AVSalgo.png" width="100%">
	</div>	
	<div class='content' style='grid-area: txt1;'>
		<p> The aim of AVS is to provide a customized summary with as
		little effort as possible from the user side. The system first
		asks for the user’s initial preferences, selected from a set
		of items, i.e. the most frequent items in the original video.
		Then, the user’s preferences are further refined through a
		question-asking inference.</p> 

		<p>
		AVS asks the user specific questions about segments of
		the video. It shows one selected segment, and asks the following
		two binary questions: 
		<ol>
		  <li>Would you want this segment
		to be in the final summary?</li>
		  <li>Would you want to
		include similar segments?</li>
		</ol>
		</p> 

		<p>
		Note that the original video is not shown to the user, as the
		segments shown during the interaction, and the subsequently generated summaries, provide an accurate
		idea of the video content in much less time.</p> 

		<p>
		Thus, AVS can be divided into two inference problems:
		<ol type="i">
		  <li>infer the customized summary, and</li>
		  <li>infer the next
		segment to show.</li>
		</ol>
		We use a probabilistic approach
		based on active inference in Conditional Random Fields
		(CRFs) to infer the most likely summary,
		and to estimate the next question to ask.</p> 
	</div>

	<div class='content' style='grid-area: txt2;'>
		<hr>
		<h2 text-align='left'>Experimental Results </h2>

            <p>We analyze two scenarios in which AVS can be used in practice.
		In the first scenario, the user has to summarize a video
		never seen before. The user has no knowledge of the video
		essence, and thus does not know yet what are the relevant
		parts. AVS allows the user to discover his or her own preferences
		while exploring the video content. Generating summaries in this scenario with AVS results
		4 times faster than doing it manually.
			</p> 

			<p> In the second scenario, the user already knows the content
		of the video (e.g. the user was the camera wearer), and
		already knows his or her preferences. However, due to the
		length of the original video, looking for such preferences in
		the video is very time consuming. AVS allows for the user
		to browse the video and find such events easier and faster. 
		To test AVS in this scenario, we gave the users a set of element to be found in the video.
		We score the summaries according to how many events appear on it.
		The results using the different baselines are the following:
                </p>
			
		<section class="figGrid">
			<div class="figure" style="grid-area: area1;">
				<img src="files/AVSresults1.png" width="100%">
			</div>
			<div class="figure" style="grid-area: area2;">
				<img src="files/AVSresultsLegend.png" width="100%">
			</div>	
			<div class="figure" style="grid-area: area3;">
				<img src="files/AVSresults2.png" width="100%">
			</div>	
		</section>
	
  </section>
</body>
</html>


